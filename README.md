# AI Hub Services

Bu proje, gÃ¶rsel Ã¼retim, nesne tespiti ve gÃ¶rselden soru-cevap servislerini iÃ§eren bir AI hub'Ä±dÄ±r.

## ğŸš€ Servisler

- **imggen** (Port 8001): SDXL-Turbo ile gÃ¶rsel Ã¼retim (GPU hÄ±zlandÄ±rmalÄ±)
- **detect** (Port 8003): Gemma3:27b ile nesne tespiti (Ollama Ã¼zerinden)
- **vqa** (Port 8002): LLaVA:34b ile gÃ¶rselden soru-cevap (Ollama Ã¼zerinden)

## ğŸ–¥ï¸ Sistem Gereksinimleri

### DonanÄ±m
- **GPU**: NVIDIA GPU (CUDA 12.1+ destekli) - ImageGen servisi iÃ§in
- **RAM**: En az 16GB (Ollama modelleri iÃ§in)
- **Disk**: En az 50GB boÅŸ alan (modeller iÃ§in)

### YazÄ±lÄ±m
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **NVIDIA Container Toolkit**: GPU desteÄŸi iÃ§in
- **Ollama**: 0.1.0+ (host sistemde)
- **CUDA**: 12.1+ (GPU iÃ§in)

## âš ï¸ Ã–nemli Notlar

### Ollama YÃ¶netimi
- **Kritik**: Ollama servisi bazen yeniden baÅŸlatÄ±lmasÄ± gerekebilir
- Detect servisi timeout verirse: `sudo systemctl restart ollama`
- Ollama host'ta Ã§alÄ±ÅŸmalÄ± (127.0.0.1:11434)
- Container'lar `network_mode: "host"` kullanÄ±r

### GPU KullanÄ±mÄ±
- **imggen**: CUDA GPU hÄ±zlandÄ±rmasÄ± kullanÄ±r
- **detect**: Ollama Ã¼zerinden Ã§alÄ±ÅŸÄ±r (GPU opsiyonel)
- **vqa**: Ollama Ã¼zerinden Ã§alÄ±ÅŸÄ±r (GPU opsiyonel)

## ğŸ› ï¸ Kurulum

### 1. Ollama Kurulumu (Host)
```bash
# Ollama'yÄ± host'ta kur ve Ã§alÄ±ÅŸtÄ±r
curl -fsSL https://ollama.ai/install.sh | sh
sudo systemctl enable ollama
sudo systemctl start ollama

# Gerekli modelleri indir
ollama pull gemma3:27b    # Detect servisi iÃ§in
ollama pull llava:34b     # VQA servisi iÃ§in
```

### 2. NVIDIA Docker Kurulumu
```bash
# NVIDIA Container Toolkit kur
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### 3. Proje Kurulumu
```bash
# Projeyi klonla
git clone <repository-url>
cd ai_hub

# Gerekli dizinleri oluÅŸtur
mkdir -p uploads outputs

# Servisleri baÅŸlat
docker-compose up --build -d
```

## ğŸ“ Dizin YapÄ±sÄ±

```
ai_hub/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ imggen/          # GÃ¶rsel Ã¼retim servisi (GPU gerekli)
â”‚   â”œâ”€â”€ detect/          # Nesne tespiti servisi (Ollama Ã¼zerinden)
â”‚   â””â”€â”€ vqa/             # VQA servisi (Ollama Ã¼zerinden)
â”œâ”€â”€ uploads/             # YÃ¼klenen dosyalar
â”œâ”€â”€ outputs/             # Ãœretilen Ã§Ä±ktÄ±lar
â”œâ”€â”€ docker-compose.yml   # Servis konfigÃ¼rasyonu
â””â”€â”€ README.md            
```

## ğŸ”§ API KullanÄ±mÄ±

### GÃ¶rsel Ãœretim (imggen)
```bash
curl -X POST http://localhost:8001/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "a beautiful sunset over mountains"}'
```

### Nesne Tespiti (detect)
```bash
curl -X POST http://localhost:8003/detect \
  -F "image=@uploads/deneme2.jpg" \
  -F "confidence=0.3" \
  -F "max_objects=15"
```

### GÃ¶rselden Soru-Cevap (vqa)
```bash
curl -X POST http://localhost:8002/vqa \
  -F "image=@uploads/deneme2.jpg" \
  -F "question=What do you see in this image?"
```

## ğŸ”„ Sistem AkÄ±ÅŸ DiyagramÄ±

```mermaid
graph TB
    A[KullanÄ±cÄ±] --> B[API Ä°stekleri]
    
    B --> C[ImageGen Servisi<br/>Port 8001]
    B --> D[Detect Servisi<br/>Port 8003]
    B --> E[VQA Servisi<br/>Port 8002]
    
    C --> F[SDXL-Turbo Model<br/>CUDA GPU]
    F --> G[GÃ¶rsel Ãœretimi<br/>outputs/ klasÃ¶rÃ¼]
    
    D --> H[Ollama API<br/>127.0.0.1:11434]
    E --> H
    
    H --> I[Gemma3:27b Model<br/>Nesne Tespiti]
    H --> J[LLaVA:34b Model<br/>GÃ¶rsel Soru-Cevap]
    
    I --> K[TÃ¼rkÃ§e Nesne Listesi<br/>JSON Response]
    J --> L[GÃ¶rsel Analiz CevabÄ±<br/>JSON Response]
    
    G --> M[KullanÄ±cÄ±ya DÃ¶nen SonuÃ§]
    K --> M
    L --> M
    
    style C fill:#e1f5fe
    style D fill:#f3e5f5
    style E fill:#e8f5e8
    style F fill:#fff3e0
    style H fill:#fce4ec
```

## ğŸ› Sorun Giderme

### Ollama Timeout HatasÄ±
```bash
# Ollama servisini yeniden baÅŸlat
sudo systemctl restart ollama

# Servis durumunu kontrol et
sudo systemctl status ollama

# Modellerin yÃ¼klÃ¼ olduÄŸunu kontrol et
ollama list
```

### GPU HatasÄ± (imggen iÃ§in)
```bash
# NVIDIA Docker Ã§alÄ±ÅŸÄ±yor mu kontrol et
docker run --rm --gpus all nvidia/cuda:12.1.1-base-ubuntu22.04 nvidia-smi

# GPU kullanÄ±mÄ±nÄ± kontrol et
nvidia-smi
```

### Port Ã‡akÄ±ÅŸmasÄ±
```bash
# Hangi portlar kullanÄ±lÄ±yor kontrol et
netstat -tlnp | grep :800

# Container'larÄ± kontrol et
docker ps
```

### Servis SaÄŸlÄ±k KontrolÃ¼
```bash
# TÃ¼m servislerin saÄŸlÄ±ÄŸÄ±nÄ± kontrol et
curl http://localhost:8001/health  # ImageGen
curl http://localhost:8002/health  # VQA
curl http://localhost:8003/health  # Detect
```

## ğŸ“ Ã–zellikler

- **TÃ¼rkÃ§e Destek**: Detect servisi TÃ¼rkÃ§e nesne tespiti yapar
- **GPU HÄ±zlandÄ±rma**: ImageGen servisi CUDA GPU kullanÄ±r
- **ModÃ¼ler YapÄ±**: Her servis baÄŸÄ±msÄ±z olarak Ã§alÄ±ÅŸabilir
- **Docker TabanlÄ±**: Kolay kurulum ve daÄŸÄ±tÄ±m
- **RESTful API**: Standart HTTP API'ler
- **Dosya YÃ¶netimi**: Otomatik upload/output yÃ¶netimi

## ğŸ”§ GeliÅŸtirme

### Yeni Servis Ekleme
1. `services/` altÄ±nda yeni klasÃ¶r oluÅŸtur
2. `Dockerfile` ve `app.py` ekle
3. `docker-compose.yml`'e servis ekle
4. Gerekli portlarÄ± ayarla

### Log KontrolÃ¼
```bash
# TÃ¼m servislerin loglarÄ±nÄ± gÃ¶r
docker-compose logs -f

# Belirli servisin loglarÄ±nÄ± gÃ¶r
docker-compose logs -f imggen
docker-compose logs -f detect
docker-compose logs -f vqa
```

## ğŸ“Š Performans

- **ImageGen**: ~2-5 saniye (GPU'ya baÄŸlÄ±)
- **Detect**: ~3-8 saniye (Ollama'ya baÄŸlÄ±)
- **VQA**: ~5-15 saniye (Ollama'ya baÄŸlÄ±)

## ğŸ¤ KatkÄ±da Bulunma

1. Fork yap
2. Feature branch oluÅŸtur
3. DeÄŸiÅŸikliklerini commit et
4. Pull request gÃ¶nder

## ğŸ“„ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r.